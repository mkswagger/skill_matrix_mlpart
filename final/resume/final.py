# -*- coding: utf-8 -*-
"""final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JciXjJCKfnDbnbMUmSHXDUBjmv4f9UUe
"""

import urllib.request

url = "https://firebasestorage.googleapis.com/v0/b/skill-matrix-2a884.appspot.com/o/Arindam%20Lahiri%20Resume%20Aug%20'21.pdf?alt=media&token=07238b80-f27f-46b1-9091-7b771355612c"
file_name = "arindam.pdf"

urllib.request.urlretrieve(url, file_name)

# !pip install pdfminer-six

from pdfminer.pdfpage import PDFPage
from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter
from pdfminer.converter import TextConverter
from pdfminer.layout import LAParams
import io
import os



filename="arindam.pdf"
i_f = open(filename,'rb')
resMgr = PDFResourceManager()
retData = io.StringIO()
TxtConverter = TextConverter(resMgr,retData, laparams= LAParams())
interpreter = PDFPageInterpreter(resMgr,TxtConverter)
for page in PDFPage.get_pages(i_f):
    interpreter.process_page(page)
    txt = retData.getvalue()
    print(txt)

# !pip install spacy

# import spacy

# # load pre-trained model
# nlp = spacy.load('en_core_web_sm')

# def extract_skill_1(resume_text):
#     nlp_text = nlp(resume_text)
#     tokens = [token.text for token in nlp_text if not token.is_stop]
    
#     # manually add skills database
#     skills = result
#     skillset = []
    
#     # one-gram skill (example: python)
#     for i in tokens:
#         if i.lower() in skills: # make every skill lowercase to match with skills database we had
#             skillset.append(i)
    
    
#     # bi-grams or tri-grams skill (example: machine learning)
#     for i in nlp_text.noun_chunks:
#         i = i.text.lower().strip()
#         if i in skills:
#             skillset.append(i)

    
#     # capitalize and remove duplicates
#     return [word.capitalize() for word in set([word.lower() for word in skillset])]


# extract_skill_1(txt)

# result = []
# with open('linkedin skill',encoding="utf-8") as f:
#     external_source = list(f)
#     for element in external_source:
#         result.append(element.strip().lower())
#     print(result)

def get_skills(document):
    skill_terms = []
    
    with open("linkedin skill", 'r',encoding="utf-8") as file:
        skill_terms = file.readlines()
    
    skill_terms = [term.strip('\n') for term in skill_terms]
    skills = []
    
    for line in document:
        words = line.split(' ')
        
        for word in words:
            if word in skill_terms:
                if word not in skills:
                    skills.append(word)
                    
        word_pairs = []
        for i in zip(words[:-1], words[1:]):
            word_pairs.append(i[0] + ' ' + i[1])   #This is to find skills like 'data science' i.e skills containint two words.    return (skills)
            
        for pair in word_pairs:
            if pair in skill_terms:
                if pair not in skills:
                    skills.append(pair)
                    
    return (skills)
     
get_skills(txt)