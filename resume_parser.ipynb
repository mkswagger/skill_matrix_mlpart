{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "44059fc2",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2022-11-21T04:53:18.599202Z",
          "iopub.status.busy": "2022-11-21T04:53:18.598737Z",
          "iopub.status.idle": "2022-11-21T04:53:18.646072Z",
          "shell.execute_reply": "2022-11-21T04:53:18.644443Z"
        },
        "papermill": {
          "duration": 0.061462,
          "end_time": "2022-11-21T04:53:18.650237",
          "exception": false,
          "start_time": "2022-11-21T04:53:18.588775",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44059fc2",
        "outputId": "face4e44-0cb2-438d-c442-0388ad394924"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/resume_parser/pt2/linkedin skill\n",
            "/content/drive/MyDrive/resume_parser/pt2/AnuvaGoyal_Latex.pdf\n",
            "/content/drive/MyDrive/resume_parser/pt2/candidate_018.pdf\n",
            "/content/drive/MyDrive/resume_parser/pt2/1901841_RESUME.pdf\n",
            "/content/drive/MyDrive/resume_parser/pt2/candidate_042.pdf\n",
            "/content/drive/MyDrive/resume_parser/pt2/candidate_056.pdf\n"
          ]
        }
      ],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/content/drive/MyDrive/resume_parser/pt2'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qYdXD9xPaPV",
        "outputId": "0bb58fae-df23-4468-af9f-8347edda1de2"
      },
      "id": "9qYdXD9xPaPV",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "0eeb7daf",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-21T04:53:18.669333Z",
          "iopub.status.busy": "2022-11-21T04:53:18.668873Z",
          "iopub.status.idle": "2022-11-21T04:53:35.215019Z",
          "shell.execute_reply": "2022-11-21T04:53:35.213750Z"
        },
        "papermill": {
          "duration": 16.558366,
          "end_time": "2022-11-21T04:53:35.217967",
          "exception": false,
          "start_time": "2022-11-21T04:53:18.659601",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eeb7daf",
        "outputId": "906b94ee-b3bb-4520-da14-8d9e88c1532c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pdfminer-six\n",
            "  Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m94.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from pdfminer-six) (2.0.12)\n",
            "Collecting cryptography>=36.0.0\n",
            "  Downloading cryptography-39.0.2-cp36-abi3-manylinux_2_28_x86_64.whl (4.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m102.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.9/dist-packages (from cryptography>=36.0.0->pdfminer-six) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer-six) (2.21)\n",
            "Installing collected packages: cryptography, pdfminer-six\n",
            "Successfully installed cryptography-39.0.2 pdfminer-six-20221105\n"
          ]
        }
      ],
      "source": [
        "!pip install pdfminer-six\n",
        "\n",
        "from pdfminer.pdfpage import PDFPage\n",
        "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
        "from pdfminer.converter import TextConverter\n",
        "from pdfminer.layout import LAParams\n",
        "import io"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6db5ac9d",
      "metadata": {
        "papermill": {
          "duration": 0.009843,
          "end_time": "2022-11-21T04:53:35.238128",
          "exception": false,
          "start_time": "2022-11-21T04:53:35.228285",
          "status": "completed"
        },
        "tags": [],
        "id": "6db5ac9d"
      },
      "source": [
        "**Parse PDF to TXT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "90394141",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-21T04:53:35.259038Z",
          "iopub.status.busy": "2022-11-21T04:53:35.258575Z",
          "iopub.status.idle": "2022-11-21T04:53:35.656250Z",
          "shell.execute_reply": "2022-11-21T04:53:35.654719Z"
        },
        "papermill": {
          "duration": 0.411531,
          "end_time": "2022-11-21T04:53:35.659076",
          "exception": false,
          "start_time": "2022-11-21T04:53:35.247545",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90394141",
        "outputId": "4b99e81f-6784-488a-b9a0-93387fc31f62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ANUVA GOYAL\n",
            "\n",
            " anuvagoyal111@gmail.com\n",
            "\n",
            " Agra, Uttar Pradesh, India\n",
            "\n",
            "(cid:135) github.com/AnuvaGoyal\n",
            "\n",
            "EXPERIENCE\n",
            "\n",
            "PROJECTS\n",
            "\n",
            "Summer Intern\n",
            "Genisup India Pvt. Ltd., Hosur, Tamil Nadu\n",
            "(cid:17) June 2021 – Aug 2021\n",
            "\n",
            " Remote\n",
            "\n",
            "• Internship on the topic NLP: Topic Modeling to assign the\n",
            "\n",
            "theme or topic for any news article on internet using Machine\n",
            "Learning techniques.\n",
            "\n",
            "• Worked on proxy rotation and Web Scraping\n",
            "\n",
            "• Mental Healthcare Chatbot that provides ad-\n",
            "vice to the user based on diﬀerent categories\n",
            "of mental health problems using a dataset\n",
            "webscraped from counselchat.com (Nov 2021)\n",
            "\n",
            "• Full stack Speech Emotion based Movie\n",
            "\n",
            "Recommender System using the RAVDESS\n",
            "Dataset and Web Scraping techniques (Oct\n",
            "2021)\n",
            "\n",
            "• Performed LDA Topic Modeling on “The Hindu” news articles\n",
            "\n",
            "• Finding a Perfect Fit, a model to parse re-\n",
            "\n",
            "and obtained precision score of 0.906.\n",
            "\n",
            "Intern Trainee\n",
            "VUGS Technologies Pvt. Ltd., Agra, Uttar Pradesh\n",
            "(cid:17) May 2021 – June 2021\n",
            "\n",
            " Remote\n",
            "\n",
            "• Built an OCR using Pytesseract and NER Text Classiﬁcation\n",
            "\n",
            "Model to categorize detected text into Name, E-mail Address,\n",
            "Phone number and Date using NLTK,SpaCy and BERT\n",
            "\n",
            "• Created an OCR for Handwritten text [A-Z, 0-9] using CNN\n",
            "\n",
            "architecture\n",
            "\n",
            "• Built a Face Recognition Model and Face Mask Detection\n",
            "\n",
            "Model using OpenCV and Haar Cascade Classiﬁer.\n",
            "\n",
            "TRAININGS\n",
            "\n",
            "Deep Learning and Computer Vision A-Z: OpenCV,\n",
            "SSD, GANs\n",
            "Udemy\n",
            "(cid:17) Nov 2021 – Dec 2021\n",
            "\n",
            "30 Days of Google Cloud\n",
            "Google LLC\n",
            "(cid:17) Oct 2021 – Nov 2021\n",
            "\n",
            "Neural Networks and Deep Learning\n",
            "Coursera\n",
            "(cid:17) Mar 2021 – Apr 2021\n",
            "\n",
            "Machine Learning A-Z: Hands on Python and R\n",
            "in Data Science\n",
            "Udemy\n",
            "(cid:17) Dec 2020 – Feb 2021\n",
            "\n",
            "ACHIEVEMENTS\n",
            "\n",
            "• Speaker at ML/AI Event organized by the Google Developers\n",
            "\n",
            "Student Club, DEI (Dec 2021)\n",
            "\n",
            "• Secured 3rd position in TECH-A-THON organized by The ECE\n",
            "\n",
            "Society, BIT Mesra, Ranchi (Oct 2021)\n",
            "\n",
            "sumes using Pytesseract, NLP and XG Boost\n",
            "and Random Forest classiﬁcation techniques\n",
            "(Aug 2021)\n",
            "\n",
            "• Face Mask Detection which detects the face\n",
            "using Haar Cascade Classiﬁer and classiﬁes\n",
            "the image into one of the three categories-\n",
            "Without Mask, With Mask and Incorrect Mask\n",
            "(May 2021)\n",
            "\n",
            "EDUCATION\n",
            "\n",
            "B.Tech, Electrical Engineering with\n",
            "Computer Science Specialisation\n",
            "Dayalbagh Educational Institute, Agra\n",
            "(cid:17) July 2019– Present (SGPA 9.35 - 4 SEM)\n",
            "\n",
            "Higher Secondary School Certiﬁcate\n",
            "St. Clare’s Senior Secondary School, Agra\n",
            "(cid:17) 2019 (94%)\n",
            "\n",
            "Secondary School Certiﬁcate\n",
            "St. Clare’s Senior Secondary School, Agra\n",
            "(cid:17) 2017 (CGPA 10)\n",
            "\n",
            "PUBLICATION\n",
            "\n",
            "researchgate.net/proﬁle/Anuva_Goyal\n",
            "\n",
            "SKILLS\n",
            "\n",
            "C, C++\n",
            "Python, SQL\n",
            "Data Structures\n",
            "CSS, HTML\n",
            "\n",
            "○ ○ ○ ○ ○\n",
            "○ ○ ○ ○ ○\n",
            "○ ○ ○ ○ ○\n",
            "○ ○ ○ ○ ○\n",
            "\n",
            "Machine Learning Frameworks: NumPy,\n",
            "Pandas, Tensorﬂow, Scikit-Learn, OpenCV,\n",
            "Pytesseract, BeautifulSoup\n",
            "\n",
            "INTERESTS\n",
            "\n",
            "• Attended online KLA Workshop on AI and HPC in Semicon-\n",
            "ductor Manufacturing organized by IIT Madras (Sep 2021)\n",
            "\n",
            "• ML and Deep Learning.\n",
            "\n",
            "• NLP and Computer Vision\n",
            "\n",
            "• Won 1st prize in the online competition Game of Brands orga-\n",
            "\n",
            "nized by SGGSCC, University of Delhi (Mar 2021)\n",
            "\n",
            "PROJECTS\n",
            "\n",
            "• Mental Healthcare Chatbot that provides ad-\n",
            "\n",
            "vice to the user based on diﬀerent categories\n",
            "\n",
            "of mental health problems using a dataset\n",
            "\n",
            "webscraped from counselchat.com (Nov 2021)\n",
            "\n",
            "• Full stack Speech Emotion based Movie\n",
            "\n",
            "Recommender System using the RAVDESS\n",
            "\n",
            "Dataset and Web Scraping techniques (Oct\n",
            "\n",
            "2021)\n",
            "\n",
            "• Finding a Perfect Fit, a model to parse re-\n",
            "\n",
            "sumes using Pytesseract, NLP and XG Boost\n",
            "\n",
            "and Random Forest classiﬁcation techniques\n",
            "\n",
            "(Aug 2021)\n",
            "\n",
            "• Face Mask Detection which detects the face\n",
            "\n",
            "using Haar Cascade Classiﬁer and classiﬁes\n",
            "\n",
            "the image into one of the three categories-\n",
            "\n",
            "Without Mask, With Mask and Incorrect Mask\n",
            "\n",
            "(May 2021)\n",
            "\n",
            "EDUCATION\n",
            "\n",
            "B.Tech, Electrical Engineering with\n",
            "\n",
            "Computer Science Specialisation\n",
            "\n",
            "Dayalbagh Educational Institute, Agra\n",
            "\n",
            "(cid:17) July 2019– Present (SGPA 9.35 - 4 SEM)\n",
            "\n",
            "Higher Secondary School Certiﬁcate\n",
            "\n",
            "St. Clare’s Senior Secondary School, Agra\n",
            "\n",
            "(cid:17) 2019 (94%)\n",
            "\n",
            "Secondary School Certiﬁcate\n",
            "\n",
            "St. Clare’s Senior Secondary School, Agra\n",
            "\n",
            "(cid:17) 2017 (CGPA 10)\n",
            "\n",
            "PUBLICATION\n",
            "\n",
            "researchgate.net/proﬁle/Anuva_Goyal\n",
            "\n",
            "SKILLS\n",
            "\n",
            "C, C++\n",
            "\n",
            "Python, SQL\n",
            "\n",
            "Data Structures\n",
            "\n",
            "CSS, HTML\n",
            "\n",
            "○ ○ ○ ○ ○\n",
            "\n",
            "○ ○ ○ ○ ○\n",
            "\n",
            "○ ○ ○ ○ ○\n",
            "\n",
            "○ ○ ○ ○ ○\n",
            "\n",
            "Machine Learning Frameworks: NumPy,\n",
            "\n",
            "Pandas, Tensorﬂow, Scikit-Learn, OpenCV,\n",
            "\n",
            "Pytesseract, BeautifulSoup\n",
            "\n",
            "INTERESTS\n",
            "\n",
            "• ML and Deep Learning.\n",
            "\n",
            "• NLP and Computer Vision\n",
            "\n",
            "\f\n"
          ]
        }
      ],
      "source": [
        "i_f = open('/content/drive/MyDrive/resume_parser/pt2/AnuvaGoyal_Latex.pdf','rb')\n",
        "resMgr = PDFResourceManager()\n",
        "retData = io.StringIO()\n",
        "TxtConverter = TextConverter(resMgr,retData, laparams= LAParams())\n",
        "interpreter = PDFPageInterpreter(resMgr,TxtConverter)\n",
        "for page in PDFPage.get_pages(i_f):\n",
        "    interpreter.process_page(page)\n",
        "    txt = retData.getvalue()\n",
        "    print(txt)\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "49572b9a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-21T04:53:35.681032Z",
          "iopub.status.busy": "2022-11-21T04:53:35.680590Z",
          "iopub.status.idle": "2022-11-21T04:53:35.690512Z",
          "shell.execute_reply": "2022-11-21T04:53:35.688350Z"
        },
        "papermill": {
          "duration": 0.024561,
          "end_time": "2022-11-21T04:53:35.693289",
          "exception": false,
          "start_time": "2022-11-21T04:53:35.668728",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49572b9a",
        "outputId": "ae4b9571-532b-4c2e-9352-05bd016345ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['anuvagoyal111@gmail.com']\n"
          ]
        }
      ],
      "source": [
        "# extract email\n",
        "extracted_text = {}\n",
        "text = txt + \"1234567891\"\n",
        "import re\n",
        "def get_email_addresses(string):\n",
        "    r = re.compile('[\\w\\.-]+\\s*@\\s*[\\w\\.-]+')\n",
        "    return r.findall(string)\n",
        "\n",
        "email = get_email_addresses(txt)\n",
        "print(email)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "f81a574e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-21T04:53:35.716253Z",
          "iopub.status.busy": "2022-11-21T04:53:35.715819Z",
          "iopub.status.idle": "2022-11-21T04:53:35.724852Z",
          "shell.execute_reply": "2022-11-21T04:53:35.723069Z"
        },
        "papermill": {
          "duration": 0.023048,
          "end_time": "2022-11-21T04:53:35.727745",
          "exception": false,
          "start_time": "2022-11-21T04:53:35.704697",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f81a574e",
        "outputId": "9ac08953-ac0d-4b5e-a161-ceb3e2ad7126"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ],
      "source": [
        "# extract phone number\n",
        "def get_phone_numbers(string):\n",
        "    r = re.compile(r'(\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{4}|\\d{3}[-\\.\\s]??\\d{4})')\n",
        "    \n",
        "    phone_numbers = r.findall(string)\n",
        "    return [re.sub(r'\\D', '', num) for num in phone_numbers]\n",
        "\n",
        "phone_number= get_phone_numbers(txt)\n",
        "print(phone_number)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "715c521d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-21T04:53:35.749233Z",
          "iopub.status.busy": "2022-11-21T04:53:35.748813Z",
          "iopub.status.idle": "2022-11-21T04:53:51.116511Z",
          "shell.execute_reply": "2022-11-21T04:53:51.115001Z"
        },
        "papermill": {
          "duration": 15.381995,
          "end_time": "2022-11-21T04:53:51.119368",
          "exception": false,
          "start_time": "2022-11-21T04:53:35.737373",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "715c521d",
        "outputId": "d2b4fec0-eb71-45d9-8578-8ca8b8dd2a33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting nlp\n",
            "  Downloading nlp-0.4.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from nlp) (3.10.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from nlp) (4.65.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from nlp) (1.4.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from nlp) (2.27.1)\n",
            "Requirement already satisfied: pyarrow>=0.16.0 in /usr/local/lib/python3.9/dist-packages (from nlp) (9.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from nlp) (1.22.4)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 KB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dill\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 KB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->nlp) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->nlp) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->nlp) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->nlp) (2.0.12)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->nlp) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->nlp) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->nlp) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, nlp\n",
            "Successfully installed dill-0.3.6 nlp-0.4.0 xxhash-3.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install nlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "699e7938",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-21T04:53:51.145068Z",
          "iopub.status.busy": "2022-11-21T04:53:51.144658Z",
          "iopub.status.idle": "2022-11-21T04:54:04.047024Z",
          "shell.execute_reply": "2022-11-21T04:54:04.045848Z"
        },
        "papermill": {
          "duration": 12.918397,
          "end_time": "2022-11-21T04:54:04.050207",
          "exception": false,
          "start_time": "2022-11-21T04:53:51.131810",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "699e7938",
        "outputId": "c4996d76-887e-4ae9-f86c-2d4bfa467609"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ANUVA GOYAL\n"
          ]
        }
      ],
      "source": [
        "# extract name\n",
        "\n",
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "# load pre-trained model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# initialize matcher with a vocab\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "\n",
        "def extract_name(resume_text):\n",
        "    nlp_text = nlp(resume_text)\n",
        "    \n",
        "    # rule: name must be consisting of 2 proper noun (only first and last name)\n",
        "    patterns = [{'POS': 'PROPN'}, {'POS': 'PROPN'}]\n",
        "    \n",
        "    matcher.add('NAME', [patterns], on_match = None)\n",
        "    \n",
        "    matches = matcher(nlp_text)\n",
        "    \n",
        "    for match_id, start, end in matches:\n",
        "        span = nlp_text[start:end]\n",
        "        return span.text\n",
        "\n",
        "name = extract_name(txt)\n",
        "print(name)\n",
        "extracted_text[\"Name\"] = name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "1c2df245",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-21T04:54:04.074775Z",
          "iopub.status.busy": "2022-11-21T04:54:04.074072Z",
          "iopub.status.idle": "2022-11-21T04:54:04.940746Z",
          "shell.execute_reply": "2022-11-21T04:54:04.939224Z"
        },
        "papermill": {
          "duration": 0.882241,
          "end_time": "2022-11-21T04:54:04.943745",
          "exception": false,
          "start_time": "2022-11-21T04:54:04.061504",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c2df245",
        "outputId": "6c1a4038-e607-43a9-eb7c-da3df1e123a1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['B.tech']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# extract edu\n",
        "\n",
        "import spacy\n",
        "import pandas as pd\n",
        "\n",
        "# load pre-trained model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "\n",
        "def extract_edu(resume_text):\n",
        "    nlp_text = nlp(resume_text)\n",
        "    \n",
        "    # word tokenization and remove stop words\n",
        "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
        "    \n",
        "    # manually add skills database\n",
        "    sets = ['b.e', 'b.tech', 'b.s', 'be', \n",
        "              'btech', 'bs', 'b.s.',\n",
        "              'bachelor of science', 'bachelor of engineering',\n",
        "              'bachelor of technology', 'bachelor of']\n",
        "    \n",
        "    extractedEdu = []\n",
        "    \n",
        "    # one-gram (example: B.E)\n",
        "    for i in tokens:\n",
        "        if i.lower() in sets: \n",
        "            extractedEdu.append(i)\n",
        "    \n",
        "    \n",
        "    # bi-grams or tri-grams (example: bachelor of science)\n",
        "    for i in nlp_text.noun_chunks:\n",
        "        i = i.text.lower().strip()\n",
        "        if i in sets:\n",
        "            extractedEdu.append(i)\n",
        "\n",
        "    \n",
        "    # capitalize and remove duplicates\n",
        "    return [word.capitalize() for word in set([word.lower() for word in extractedEdu])]\n",
        "    \n",
        "    \n",
        "\n",
        "extract_edu(txt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "4caf86e3",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-21T04:54:04.968276Z",
          "iopub.status.busy": "2022-11-21T04:54:04.967890Z",
          "iopub.status.idle": "2022-11-21T04:54:05.008933Z",
          "shell.execute_reply": "2022-11-21T04:54:05.007976Z"
        },
        "papermill": {
          "duration": 0.056271,
          "end_time": "2022-11-21T04:54:05.011331",
          "exception": false,
          "start_time": "2022-11-21T04:54:04.955060",
          "status": "completed"
        },
        "tags": [],
        "id": "4caf86e3"
      },
      "outputs": [],
      "source": [
        "# load external database\n",
        "result = []\n",
        "with open('/content/drive/MyDrive/resume_parser/pt2/linkedin skill') as f:\n",
        "    external_source = list(f)\n",
        "    \n",
        "for element in external_source:\n",
        "    result.append(element.strip().lower())\n",
        "    \n",
        "#print(result)\n",
        "#external_source = open(\"/kaggle/input/employment-skills/linkedin skill\", \"r\")\n",
        "\n",
        "#def Convert(string):\n",
        " #   li = list(string.split(\" \"))\n",
        "  #  return li\n",
        "#external_source = Convert(external_source)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "0e27b6f7",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-21T04:54:05.037716Z",
          "iopub.status.busy": "2022-11-21T04:54:05.036947Z",
          "iopub.status.idle": "2022-11-21T04:54:07.026621Z",
          "shell.execute_reply": "2022-11-21T04:54:07.024805Z"
        },
        "papermill": {
          "duration": 2.005189,
          "end_time": "2022-11-21T04:54:07.029259",
          "exception": false,
          "start_time": "2022-11-21T04:54:05.024070",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e27b6f7",
        "outputId": "f7f0fa08-b28b-41fe-f19f-0c0117892c7c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Numpy',\n",
              " 'Iit',\n",
              " 'Ml',\n",
              " 'Pandas',\n",
              " 'Parse',\n",
              " 'Tamil',\n",
              " 'Coursera',\n",
              " 'Mail',\n",
              " 'Python',\n",
              " 'Opencv',\n",
              " 'Healthcare',\n",
              " 'Sem',\n",
              " 'Lda',\n",
              " 'Developers',\n",
              " 'Css',\n",
              " 'Fit',\n",
              " 'Sql',\n",
              " 'Speech',\n",
              " 'Google',\n",
              " 'Health',\n",
              " 'Deep learning',\n",
              " 'Nltk',\n",
              " 'Html',\n",
              " 'R',\n",
              " 'Web scraping',\n",
              " 'Internet',\n",
              " 'Cloud',\n",
              " 'Computer vision',\n",
              " 'Scikit-learn',\n",
              " 'Learning',\n",
              " 'Boost',\n",
              " 'Forest',\n",
              " 'Online',\n",
              " 'C',\n",
              " 'Mar',\n",
              " 'Architecture',\n",
              " 'Nlp',\n",
              " 'Mask',\n",
              " 'Articles',\n",
              " 'Modeling',\n",
              " 'Llc',\n",
              " 'Proxy',\n",
              " 'Cascade',\n",
              " 'Manufacturing',\n",
              " 'C++',\n",
              " 'Web']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "## Extract Skill Ver 2.0 ##\n",
        "\n",
        "import spacy\n",
        "import pandas as pd\n",
        "\n",
        "# load pre-trained model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "result = []\n",
        "with open('/content/drive/MyDrive/resume_parser/pt2/linkedin skill') as f:\n",
        "    external_source = list(f)\n",
        "    \n",
        "for element in external_source:\n",
        "    result.append(element.strip().lower())\n",
        "\n",
        "import spacy\n",
        "\n",
        "# load pre-trained model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def extract_skill_1(resume_text):\n",
        "    nlp_text = nlp(resume_text)\n",
        "    \n",
        "    # word tokenization and remove stop words\n",
        "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
        "    \n",
        "    # manually add skills database\n",
        "    skills = result\n",
        "    skillset = []\n",
        "    \n",
        "    # one-gram skill (example: python)\n",
        "    for i in tokens:\n",
        "        if i.lower() in skills: # make every skill lowercase to match with skills database we had\n",
        "            skillset.append(i)\n",
        "    \n",
        "    \n",
        "    # bi-grams or tri-grams skill (example: machine learning)\n",
        "    for i in nlp_text.noun_chunks:\n",
        "        i = i.text.lower().strip()\n",
        "        if i in skills:\n",
        "            skillset.append(i)\n",
        "\n",
        "    \n",
        "    # capitalize and remove duplicates\n",
        "    return [word.capitalize() for word in set([word.lower() for word in skillset])]\n",
        "\n",
        "\n",
        "extract_skill_1(txt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b974c469",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-21T04:54:07.055436Z",
          "iopub.status.busy": "2022-11-21T04:54:07.054965Z",
          "iopub.status.idle": "2022-11-21T04:54:07.060621Z",
          "shell.execute_reply": "2022-11-21T04:54:07.058882Z"
        },
        "papermill": {
          "duration": 0.022576,
          "end_time": "2022-11-21T04:54:07.063597",
          "exception": false,
          "start_time": "2022-11-21T04:54:07.041021",
          "status": "completed"
        },
        "tags": [],
        "id": "b974c469"
      },
      "outputs": [],
      "source": [
        "#f = open(\"/kaggle/input/employment-skills/linkedin skill\", \"r\")\n",
        "#print(f.read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "875a7f31",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-21T04:54:07.089277Z",
          "iopub.status.busy": "2022-11-21T04:54:07.088832Z",
          "iopub.status.idle": "2022-11-21T04:54:08.603093Z",
          "shell.execute_reply": "2022-11-21T04:54:08.602014Z"
        },
        "papermill": {
          "duration": 1.530826,
          "end_time": "2022-11-21T04:54:08.606290",
          "exception": false,
          "start_time": "2022-11-21T04:54:07.075464",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "875a7f31",
        "outputId": "e637bdfd-18a0-456b-e663-419d1b1d7ef9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('BTech', '2021')]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "import re\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# load pre-trained model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Grad all general stop words\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "# Education Degrees\n",
        "EDUCATION = [\n",
        "            'BE','B.E.', 'B.E', 'BS', 'B.S', \n",
        "            'ME', 'M.E', 'M.E.', 'MS', 'M.S', \n",
        "            'BTECH', 'B.TECH', 'M.TECH', 'MTECH', \n",
        "            'SSC', 'HSC', 'CBSE', 'ICSE', 'BACHELOROFSCIENCE',\n",
        "    'Bachelor of Science', 'BACHELOR OF SCIENCE'\n",
        "        ]\n",
        "\n",
        "def extract_education(resume_text):\n",
        "    nlp_text = nlp(resume_text)\n",
        "\n",
        "    # Sentence Tokenizer\n",
        "    nlp_text = [sent.text.strip() for sent in nlp_text.sents]\n",
        "\n",
        "    edu = {}\n",
        "    # Extract education degree\n",
        "    for index, text in enumerate(nlp_text):\n",
        "        for tex in text.split():\n",
        "            # Replace all special symbols\n",
        "            tex = re.sub(r'[?|$|.|!|,]', r'', tex)\n",
        "            if tex.upper() in EDUCATION and tex not in STOPWORDS:\n",
        "                edu[tex] = text + nlp_text[index + 1]\n",
        "\n",
        "    education = []\n",
        "    for key in edu.keys():\n",
        "        year = re.search(re.compile(r'(((20|19)(\\d{2})))'), edu[key])\n",
        "        if year:\n",
        "            education.append((key, ''.join(year[0])))\n",
        "        else:\n",
        "            education.append(key)\n",
        "    return education\n",
        "\n",
        "\n",
        "education = extract_education(txt)\n",
        "education\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "d9d841da",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-21T04:54:08.634291Z",
          "iopub.status.busy": "2022-11-21T04:54:08.633016Z",
          "iopub.status.idle": "2022-11-21T04:54:09.714712Z",
          "shell.execute_reply": "2022-11-21T04:54:09.713508Z"
        },
        "papermill": {
          "duration": 1.098559,
          "end_time": "2022-11-21T04:54:09.717238",
          "exception": false,
          "start_time": "2022-11-21T04:54:08.618679",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9d841da",
        "outputId": "3680ceb5-2a02-4829-cd0c-871ad1c9ab53"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Css', 'Pandas', 'Deep learning', 'Python', 'Opencv']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "# extract skillset\n",
        "\n",
        "import spacy\n",
        "\n",
        "# load pre-trained model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def extract_skill(resume_text):\n",
        "    nlp_text = nlp(resume_text)\n",
        "    \n",
        "    # word tokenization and remove stop words\n",
        "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
        "    \n",
        "    # manually add skills database\n",
        "    skills = ['python', 'machine learning', \n",
        "               'css', 'C++', 'data science',\n",
        "               'PHP', 'mySQL', 'HTML', 'SQL',\n",
        "             'tensorflow', 'deep learning',\n",
        "             'pandas', 'opencv', 'typescript', 'c#',\n",
        "              'data factory', 'ci/cd']\n",
        "    \n",
        "    skillset = []\n",
        "    \n",
        "    # one-gram skill (example: python)\n",
        "    for i in tokens:\n",
        "        if i.lower() in skills: # make every skill lowercase to match with skills database we had\n",
        "            skillset.append(i)\n",
        "    \n",
        "    \n",
        "    # bi-grams or tri-grams skill (example: machine learning)\n",
        "    for i in nlp_text.noun_chunks:\n",
        "        i = i.text.lower().strip()\n",
        "        if i in skills:\n",
        "            skillset.append(i)\n",
        "\n",
        "    \n",
        "    # capitalize and remove duplicates\n",
        "    return [word.capitalize() for word in set([word.lower() for word in skillset])]\n",
        "    \n",
        "    \n",
        "\n",
        "extract_skill(txt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "a9bbefdb",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-21T04:54:09.743933Z",
          "iopub.status.busy": "2022-11-21T04:54:09.743218Z",
          "iopub.status.idle": "2022-11-21T04:54:10.569169Z",
          "shell.execute_reply": "2022-11-21T04:54:10.567890Z"
        },
        "papermill": {
          "duration": 0.842823,
          "end_time": "2022-11-21T04:54:10.571831",
          "exception": false,
          "start_time": "2022-11-21T04:54:09.729008",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9bbefdb",
        "outputId": "e369e8eb-5391-4480-a248-43352b3e7bb7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['B.tech']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "# extract degree\n",
        "\n",
        "import re\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# load pre-trained model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# get all general stop words\n",
        "# STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "DEGREES = ['b.e', 'b.tech', 'b.s', 'be', \n",
        "              'btech', 'bs', 'b.s.',\n",
        "              'bachelor of science', 'bachelor of engineering',\n",
        "              'bachelor of technology', 'bachelor of']\n",
        "\n",
        "\n",
        "def extract_degree1(resume_text):\n",
        "    nlp_text = nlp(resume_text)\n",
        "    \n",
        "    # word tokenization and remove stop words\n",
        "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
        "    \n",
        "    \n",
        "    \n",
        "    degree = []\n",
        "    \n",
        "    for i in tokens:\n",
        "        if i.lower() in DEGREES:\n",
        "            degree.append(i)\n",
        "    \n",
        "    for i in nlp_text.noun_chunks:\n",
        "        i = i.text.lower().strip()\n",
        "        if i in DEGREES:\n",
        "            degree.append(i)\n",
        "\n",
        "            \n",
        "    return [word.capitalize() for word in set([word.lower() for word in degree])]\n",
        "\n",
        "\n",
        "\n",
        "extract_degree1(txt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "2a5d574e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-21T04:54:10.598515Z",
          "iopub.status.busy": "2022-11-21T04:54:10.597864Z",
          "iopub.status.idle": "2022-11-21T04:54:10.623141Z",
          "shell.execute_reply": "2022-11-21T04:54:10.621961Z"
        },
        "papermill": {
          "duration": 0.041314,
          "end_time": "2022-11-21T04:54:10.625495",
          "exception": false,
          "start_time": "2022-11-21T04:54:10.584181",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2a5d574e",
        "outputId": "f5769e14-241f-4110-948b-1479c00a88ec"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Dayalbagh Educational Institute',\n",
              " 'Higher Secondary School',\n",
              " 's Senior Secondary School',\n",
              " 'Secondary School',\n",
              " ' University']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "# extract school using regular expression (ver 1)\n",
        "\n",
        "import re\n",
        "def extract_school(resume_txt):\n",
        "    sub_patterns = [\n",
        "                    '[A-Za-zÀ-ȕ]* University', \n",
        "                    '[A-Za-zÀ-ȕ]* [A-Za-zÀ-ȕ]* University', \n",
        "                    '[A-Za-zÀ-ȕ]* [A-Za-zÀ-ȕ]* [A-Za-zÀ-ȕ]* University', \n",
        "                    #'[A-Za-zÀ-ȕ]* [A-Za-zÀ-ȕ]* [A-Za-zÀ-ȕ]* [A-Za-zÀ-ȕ]* University',\n",
        "                    '[A-Za-zÀ-ȕ]* Institute [A-Za-zÀ-ȕ]* [A-Za-zÀ-ȕ]*',\n",
        "                    '[A-Za-zÀ-ȕ]* [A-Za-zÀ-ȕ]* Institute *[A-Za-zÀ-ȕ] *[A-Za-zÀ-ȕ]',            \n",
        "                    '[A-Za-zÀ-ȕ]* Institute',\n",
        "                    '[A-Za-zÀ-ȕ]* [A-Za-zÀ-ȕ]* Institute',\n",
        "                    '[A-Za-zÀ-ȕ]* [A-Za-zÀ-ȕ]* [A-Za-zÀ-ȕ]* Institute',\n",
        "                    'Institute [A-Za-zÀ-ȕ]* [A-Za-zÀ-ȕ]*', \n",
        "                    'Institute [A-Za-zÀ-ȕ]* [A-Za-zÀ-ȕ]*',\n",
        "                    'Institute [A-Za-zÀ-ȕ]*',    \n",
        "                    'University *[A-Za-zÀ-ȕ] [A-Za-zÀ-ȕ]*',\n",
        "                    'University [A-Za-zÀ-ȕ]* [A-Za-zÀ-ȕ]*',\n",
        "                    'University [A-Za-zÀ-ȕ]*',\n",
        "                    '[A-Za-zÀ-ȕ]* School', \n",
        "                    '[A-Za-zÀ-ȕ]* [A-Z][a-z]* School', \n",
        "                    '[A-Za-zÀ-ȕ]* [A-Za-zÀ-ȕ]* [A-Za-zÀ-ȕ]* School']\n",
        "    pattern = '({})'.format('|'.join(sub_patterns))\n",
        "    matches = re.findall(pattern, resume_txt)\n",
        "    return(list(dict.fromkeys(matches)))\n",
        "\n",
        "extract_school(txt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "71592d39",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-21T04:54:10.652454Z",
          "iopub.status.busy": "2022-11-21T04:54:10.651674Z",
          "iopub.status.idle": "2022-11-21T04:54:10.696297Z",
          "shell.execute_reply": "2022-11-21T04:54:10.694874Z"
        },
        "papermill": {
          "duration": 0.061822,
          "end_time": "2022-11-21T04:54:10.699002",
          "exception": false,
          "start_time": "2022-11-21T04:54:10.637180",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71592d39",
        "outputId": "3226eced-c1e2-4a09-d100-e40ad3e39beb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['University of Delhi']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "# extract school name using external database (ver 2)\n",
        "\n",
        "# import World University Ranking data\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/resume_parser/pt2/WORLD UNIVERSITY RANKINGS.csv')\n",
        "\n",
        "school_name = list(df['Institution'])\n",
        "\n",
        "# txt = 'Harvard University ' + txt\n",
        "\n",
        "# search and match school name with the database\n",
        "def match_school(resume_text):\n",
        "    school = []\n",
        "    \n",
        "    i = 0\n",
        "    for i in range (len(school_name)):\n",
        "        if school_name[i] in resume_text:\n",
        "            school.append(school_name[i])\n",
        "            i = i + 1\n",
        "        else:\n",
        "            i = i + 1\n",
        "    \n",
        "    return(school)\n",
        "\n",
        "match_school(txt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "38fd2271",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-21T04:54:10.725971Z",
          "iopub.status.busy": "2022-11-21T04:54:10.725573Z",
          "iopub.status.idle": "2022-11-21T04:54:11.495198Z",
          "shell.execute_reply": "2022-11-21T04:54:11.493902Z"
        },
        "papermill": {
          "duration": 0.786791,
          "end_time": "2022-11-21T04:54:11.498024",
          "exception": false,
          "start_time": "2022-11-21T04:54:10.711233",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38fd2271",
        "outputId": "a69ec55c-5956-4109-d7af-f5110ebeb300"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'email': ['anuvagoyal111@gmail.com'],\n",
              " 'phone_number': ['9520349542'],\n",
              " 'name': 'ANUVA GOYAL',\n",
              " 'skill': ['Machine learning',\n",
              "  'Data science',\n",
              "  'Css',\n",
              "  'Tensorflow',\n",
              "  'Deep learning',\n",
              "  'Python',\n",
              "  'Opencv'],\n",
              " 'skill_1': ['Iit',\n",
              "  'Parse',\n",
              "  'Drama',\n",
              "  'Volunteering',\n",
              "  'Fit',\n",
              "  'Javascript',\n",
              "  'X',\n",
              "  'Cloud',\n",
              "  'Pycharm',\n",
              "  'Github',\n",
              "  'Boost',\n",
              "  'Hero',\n",
              "  'Scheme',\n",
              "  'S7',\n",
              "  'Remote sensing',\n",
              "  'Algorithms',\n",
              "  'Word',\n",
              "  'Articles',\n",
              "  'Resumes',\n",
              "  'Cascade',\n",
              "  'Numpy',\n",
              "  'O',\n",
              "  'Novels',\n",
              "  'Healthcare',\n",
              "  'Lda',\n",
              "  'Hobbies',\n",
              "  'Sql',\n",
              "  'Google',\n",
              "  'Html',\n",
              "  'Camp',\n",
              "  'Google cloud',\n",
              "  'C',\n",
              "  'Mar',\n",
              "  'Architecture',\n",
              "  'Web',\n",
              "  'Insurance',\n",
              "  'Office',\n",
              "  'Semiconductor',\n",
              "  'Turbo c++',\n",
              "  'Coursera',\n",
              "  'Mail',\n",
              "  'Matlab',\n",
              "  'Formulation',\n",
              "  'Prediction',\n",
              "  'Css',\n",
              "  'Java',\n",
              "  'Speech',\n",
              "  'Scikit-learn',\n",
              "  'Organization',\n",
              "  'Online',\n",
              "  'Campus',\n",
              "  'Completion',\n",
              "  'Keras',\n",
              "  'Nlp',\n",
              "  'Mask',\n",
              "  'Nss',\n",
              "  'Characters',\n",
              "  'Utilization',\n",
              "  'Python',\n",
              "  'Opencv',\n",
              "  'Machine learning',\n",
              "  'Crisis',\n",
              "  'Computer science',\n",
              "  'Events',\n",
              "  'Deep learning',\n",
              "  'Analytics',\n",
              "  'R',\n",
              "  'Turbo',\n",
              "  'Data science',\n",
              "  'Internships',\n",
              "  'Learning',\n",
              "  'Forest',\n",
              "  'Classification',\n",
              "  'Workshops',\n",
              "  'Modeling',\n",
              "  'Proxy',\n",
              "  'Manufacturing',\n",
              "  'C++',\n",
              "  'Email'],\n",
              " 'degree': [('BTech', '2023'), ('MS', '2021')],\n",
              " 'sch': ['Institute ',\n",
              "  'Dayalbagh Educational Institute',\n",
              "  's Senior Secondary School',\n",
              "  ' University',\n",
              "  'Indian Institute of Remote']}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "## sample 1\n",
        "\n",
        "i_f = open('/content/drive/MyDrive/resume_parser/pt2/1901841_RESUME.pdf','rb')\n",
        "resMgr = PDFResourceManager()\n",
        "retData = io.StringIO()\n",
        "TxtConverter = TextConverter(resMgr,retData, laparams= LAParams())\n",
        "interpreter = PDFPageInterpreter(resMgr,TxtConverter)\n",
        "for page in PDFPage.get_pages(i_f):\n",
        "    interpreter.process_page(page)\n",
        "    txt = retData.getvalue()\n",
        "    \n",
        "def extractResume(resume_text):\n",
        "    \n",
        "    email_address = get_email_addresses(resume_text)\n",
        "    phone_number = get_phone_numbers(resume_text)\n",
        "    name = extract_name(resume_text)\n",
        "    skill = extract_skill(resume_text)\n",
        "    skill_from_external = extract_skill_1(resume_text)\n",
        "    degree = extract_education(resume_text)\n",
        "    sch = extract_school(resume_text)\n",
        "\n",
        "    getResult = {}\n",
        "    getResult['email'] = email_address\n",
        "    getResult['phone_number'] = phone_number\n",
        "    getResult['name'] = name\n",
        "    getResult['skill'] = skill\n",
        "    getResult['skill_1'] = skill_from_external\n",
        "    getResult['degree'] = degree\n",
        "    getResult['sch'] = sch\n",
        "    return(getResult)\n",
        "# print(txt)\n",
        "\n",
        "extractResume(txt)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "7d5f62f7",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-21T04:54:11.524235Z",
          "iopub.status.busy": "2022-11-21T04:54:11.523830Z",
          "iopub.status.idle": "2022-11-21T04:54:12.332344Z",
          "shell.execute_reply": "2022-11-21T04:54:12.330611Z"
        },
        "papermill": {
          "duration": 0.824786,
          "end_time": "2022-11-21T04:54:12.335094",
          "exception": false,
          "start_time": "2022-11-21T04:54:11.510308",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d5f62f7",
        "outputId": "7e05eb7a-1c3b-41c5-e04f-09fae4b93462"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'email': ['hello@xinni.co'],\n",
              " 'phone_number': ['6692628183'],\n",
              " 'name': 'N C',\n",
              " 'skill': ['Css'],\n",
              " 'skill_1': ['O',\n",
              "  'C++',\n",
              "  'Illustration',\n",
              "  'Iterative',\n",
              "  'App',\n",
              "  'Game design',\n",
              "  'N',\n",
              "  'Project',\n",
              "  'Testing',\n",
              "  'Prototyping',\n",
              "  'Showcase',\n",
              "  'Studio',\n",
              "  'Css',\n",
              "  'Java',\n",
              "  'Efficiency analysis',\n",
              "  'Javascript',\n",
              "  'Google',\n",
              "  'Bridging',\n",
              "  'Sql',\n",
              "  'Suite',\n",
              "  'Dissertation',\n",
              "  'Design',\n",
              "  'X',\n",
              "  'Html',\n",
              "  'R',\n",
              "  'Logging',\n",
              "  'Application',\n",
              "  'Branding',\n",
              "  'Research',\n",
              "  'Sketch',\n",
              "  'Excel',\n",
              "  'Engineers',\n",
              "  'Human computer interaction',\n",
              "  'Computing',\n",
              "  'Jquery',\n",
              "  'Experience design',\n",
              "  'Principle',\n",
              "  'Mobile design',\n",
              "  'Teaching',\n",
              "  'Ida',\n",
              "  'A+',\n",
              "  'C',\n",
              "  'Balsamiq',\n",
              "  'Agile',\n",
              "  'Man',\n",
              "  'Invision',\n",
              "  'Qt',\n",
              "  'Craft',\n",
              "  'Firebase',\n",
              "  'Usability',\n",
              "  'User research',\n",
              "  'Mobile',\n",
              "  'Competitive analysis',\n",
              "  'Communications',\n",
              "  'Literature',\n",
              "  'Angular',\n",
              "  'Information systems',\n",
              "  'D',\n",
              "  'Ionic',\n",
              "  'Award',\n",
              "  'Documentation',\n",
              "  'Adobe',\n",
              "  'Wireframing',\n",
              "  'Mockups',\n",
              "  'Web'],\n",
              " 'degree': [],\n",
              " 'sch': ['NUS School', 'National University']}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "## sample 2\n",
        "# name prediction false\n",
        "\n",
        "i_f_1 = open('/content/drive/MyDrive/resume_parser/inputs/Xinni_Chng.pdf','rb')\n",
        "resMgr = PDFResourceManager()\n",
        "retData = io.StringIO()\n",
        "TxtConverter = TextConverter(resMgr,retData, laparams= LAParams())\n",
        "interpreter = PDFPageInterpreter(resMgr,TxtConverter)\n",
        "for page in PDFPage.get_pages(i_f_1):\n",
        "    interpreter.process_page(page)\n",
        "    txt_1 = retData.getvalue()\n",
        "    \n",
        "extractResume(txt_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "0ccaa872",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-21T04:54:12.362254Z",
          "iopub.status.busy": "2022-11-21T04:54:12.361809Z",
          "iopub.status.idle": "2022-11-21T04:54:13.396214Z",
          "shell.execute_reply": "2022-11-21T04:54:13.395116Z"
        },
        "papermill": {
          "duration": 1.051127,
          "end_time": "2022-11-21T04:54:13.398747",
          "exception": false,
          "start_time": "2022-11-21T04:54:12.347620",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ccaa872",
        "outputId": "951a409d-4599-48af-b2bb-e21130f9e2a3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'email': ['arindamlahiri17@gmail.com'],\n",
              " 'phone_number': [],\n",
              " 'name': 'Arindam Lahiri',\n",
              " 'skill': ['Css'],\n",
              " 'skill_1': ['Web development',\n",
              "  'Digital',\n",
              "  'Base',\n",
              "  'Chat',\n",
              "  'Ngo',\n",
              "  'Templating',\n",
              "  'Donations',\n",
              "  'Mongodb',\n",
              "  'App',\n",
              "  'Srm',\n",
              "  'Web',\n",
              "  'Css',\n",
              "  'Forms',\n",
              "  'Javascript',\n",
              "  'Events',\n",
              "  'Cse',\n",
              "  'Html',\n",
              "  'Atom',\n",
              "  'Workspace',\n",
              "  'Github',\n",
              "  'Features',\n",
              "  'Collaboration',\n",
              "  'Video',\n",
              "  'Linkedin',\n",
              "  'Multimedia',\n",
              "  'Sdk'],\n",
              " 'degree': [('BTech', '2020')],\n",
              " 'sch': []}"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "## sample 3\n",
        "\n",
        "i_f_2 = open(\"/content/drive/MyDrive/resume_parser/pt2/Arindam Lahiri Resume Aug '21.pdf\",'rb')\n",
        "resMgr = PDFResourceManager()\n",
        "retData = io.StringIO()\n",
        "TxtConverter = TextConverter(resMgr,retData, laparams= LAParams())\n",
        "interpreter = PDFPageInterpreter(resMgr,TxtConverter)\n",
        "for page in PDFPage.get_pages(i_f_2):\n",
        "    interpreter.process_page(page)\n",
        "    txt_2 = retData.getvalue()\n",
        "    \n",
        "extractResume(txt_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "7a85f117",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-21T04:54:13.425914Z",
          "iopub.status.busy": "2022-11-21T04:54:13.425505Z",
          "iopub.status.idle": "2022-11-21T04:54:15.007488Z",
          "shell.execute_reply": "2022-11-21T04:54:15.006309Z"
        },
        "papermill": {
          "duration": 1.599235,
          "end_time": "2022-11-21T04:54:15.010957",
          "exception": false,
          "start_time": "2022-11-21T04:54:13.411722",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7a85f117",
        "outputId": "8f4af9f3-e617-437e-b71a-6a792994097d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'email': ['sn5675@srmist.edu.in'],\n",
              " 'phone_number': ['1911004010', '7903386743', '7903386743'],\n",
              " 'name': 'ACADEMIC DETAILS',\n",
              " 'skill': ['Typescript'],\n",
              " 'skill_1': ['Components',\n",
              "  'C programming',\n",
              "  'Teams',\n",
              "  'Form',\n",
              "  'Javascript',\n",
              "  'X',\n",
              "  'Android',\n",
              "  'Cloud',\n",
              "  'Github',\n",
              "  'Express',\n",
              "  'Led',\n",
              "  'Microsoft',\n",
              "  'Tokens',\n",
              "  'Git',\n",
              "  'Aws',\n",
              "  'Bootstrap',\n",
              "  'Algorithms',\n",
              "  'Nginx',\n",
              "  'Redux',\n",
              "  'Status',\n",
              "  'Registration',\n",
              "  'Kotlin',\n",
              "  'Mint',\n",
              "  'Payments',\n",
              "  'Mongodb',\n",
              "  'Cloud computing',\n",
              "  'App',\n",
              "  'Figma',\n",
              "  'Pcb',\n",
              "  'Sql',\n",
              "  'Google',\n",
              "  'Markdown',\n",
              "  'Html',\n",
              "  'Unity',\n",
              "  'Passport',\n",
              "  '3d',\n",
              "  'Technology',\n",
              "  'Computing',\n",
              "  'Languages',\n",
              "  'Typescript',\n",
              "  'B2b',\n",
              "  'Scheduling',\n",
              "  'C',\n",
              "  'Birth',\n",
              "  'Ltd',\n",
              "  'Fundamentals',\n",
              "  'Mar',\n",
              "  'Architecture',\n",
              "  'Visit',\n",
              "  'Japanese',\n",
              "  'Carousels',\n",
              "  'Ecosystem',\n",
              "  'Mockups',\n",
              "  'Web',\n",
              "  'Api',\n",
              "  'Nand',\n",
              "  'Availability',\n",
              "  'Node.js',\n",
              "  'Srm',\n",
              "  'Java',\n",
              "  'Prs',\n",
              "  'Principles',\n",
              "  'Pagination',\n",
              "  'Infrastructure',\n",
              "  'Application',\n",
              "  'React.js',\n",
              "  'Flow',\n",
              "  'Mechanics',\n",
              "  'Conferences',\n",
              "  'Campus',\n",
              "  'Features',\n",
              "  'Ux',\n",
              "  'Firebase',\n",
              "  'Mobile',\n",
              "  'Ieee',\n",
              "  'Bose',\n",
              "  'Live',\n",
              "  'Sass',\n",
              "  'Sheets',\n",
              "  'Socket.io',\n",
              "  'Express.js',\n",
              "  'Events',\n",
              "  'Object-oriented programming',\n",
              "  'Design',\n",
              "  'Analytics',\n",
              "  'Html5',\n",
              "  'Hindi',\n",
              "  'File handling',\n",
              "  'Learning',\n",
              "  'Jquery',\n",
              "  'Workshops',\n",
              "  'English',\n",
              "  'Road',\n",
              "  'C++',\n",
              "  'Google sheets',\n",
              "  'Seo',\n",
              "  'Email'],\n",
              " 'degree': [('BTech', '1911'), ('CBSE', '1911')],\n",
              " 'sch': ['s Secondary School', 'SRM Institute of Science']}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "## sample 4\n",
        "\n",
        "i_f_3 = open('/content/drive/MyDrive/resume_parser/pt2/Snehil_RA1911004010432.pdf','rb')\n",
        "resMgr = PDFResourceManager()\n",
        "retData = io.StringIO()\n",
        "TxtConverter = TextConverter(resMgr,retData, laparams= LAParams())\n",
        "interpreter = PDFPageInterpreter(resMgr,TxtConverter)\n",
        "for page in PDFPage.get_pages(i_f_3):\n",
        "    interpreter.process_page(page)\n",
        "    txt_3 = retData.getvalue()\n",
        "    \n",
        "extractResume(txt_3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "8cca39e8",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-21T04:54:19.519740Z",
          "iopub.status.busy": "2022-11-21T04:54:19.519327Z",
          "iopub.status.idle": "2022-11-21T04:54:32.360615Z",
          "shell.execute_reply": "2022-11-21T04:54:32.358675Z"
        },
        "papermill": {
          "duration": 12.858644,
          "end_time": "2022-11-21T04:54:32.363389",
          "exception": false,
          "start_time": "2022-11-21T04:54:19.504745",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cca39e8",
        "outputId": "2bd975b3-cf3d-4288-979c-43384361bd7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting nameparser\n",
            "  Downloading nameparser-1.1.2-py2.py3-none-any.whl (24 kB)\n",
            "Installing collected packages: nameparser\n",
            "Successfully installed nameparser-1.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install nameparser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "ba2ed030",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-21T04:54:32.392873Z",
          "iopub.status.busy": "2022-11-21T04:54:32.392443Z",
          "iopub.status.idle": "2022-11-21T04:54:32.833764Z",
          "shell.execute_reply": "2022-11-21T04:54:32.831863Z"
        },
        "papermill": {
          "duration": 0.460322,
          "end_time": "2022-11-21T04:54:32.837547",
          "exception": false,
          "start_time": "2022-11-21T04:54:32.377225",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba2ed030",
        "outputId": "391f60df-3ea7-4194-e7eb-4ce86c2ee1df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type:  ORGANIZATION Name:  ANUVA \n",
            "Type:  ORGANIZATION Name:  GOYAL \n",
            "Type:  PERSON Name:  Female OBJECTIVE Energetic \n",
            "Type:  PERSON Name:  Machine Learning \n",
            "Type:  ORGANIZATION Name:  NLP \n",
            "Type:  PERSON Name:  Deep Learning \n",
            "Type:  ORGANIZATION Name:  EDUCATION Qualification \n",
            "Type:  ORGANIZATION Name:  Electrical Engineering Specialization \n",
            "Type:  ORGANIZATION Name:  Computer Science \n",
            "Type:  ORGANIZATION Name:  XII \n",
            "Type:  ORGANIZATION Name:  INTERNSHIPS \n",
            "Type:  ORGANIZATION Name:  TRAININGS \n",
            "Type:  PERSON Name:  Institute Dayalbagh Educational Institute \n",
            "Type:  PERSON Name:  Dayalbagh \n",
            "Type:  PERSON Name:  Agra St. Clare \n",
            "Type:  ORGANIZATION Name:  Senior \n",
            "Type:  PERSON Name:  Agra St. Clare \n",
            "Type:  ORGANIZATION Name:  Senior \n",
            "Type:  PERSON Name:  Agra CGPA \n",
            "Type:  ORGANIZATION Name:  Completion \n",
            "Type:  ORGANIZATION Name:  Genisup India Pvt \n",
            "Type:  PERSON Name:  Learnt Web \n",
            "Type:  ORGANIZATION Name:  LDA Topic \n",
            "Type:  ORGANIZATION Name:  Hindu \n",
            "Type:  ORGANIZATION Name:  VUGS Technologies \n",
            "Type:  PERSON Name:  Pytesseract \n",
            "Type:  ORGANIZATION Name:  NER Text \n",
            "Type:  GPE Name:  Name \n",
            "Type:  PERSON Name:  Phone \n",
            "Type:  ORGANIZATION Name:  Date \n",
            "Type:  PERSON Name:  Deep \n",
            "Type:  ORGANIZATION Name:  Computer Vision \n",
            "Type:  ORGANIZATION Name:  SSD \n",
            "Type:  ORGANIZATION Name:  GANs \n",
            "Type:  GPE Name:  Udemy \n",
            "Type:  ORGANIZATION Name:  Neural Networks \n",
            "Type:  PERSON Name:  Deep Learning \n",
            "Type:  GPE Name:  Coursera \n",
            "Type:  PERSON Name:  Undertook Machine Learning \n",
            "Type:  PERSON Name:  Python \n",
            "Type:  GPE Name:  Data \n",
            "Type:  GPE Name:  Udemy \n",
            "Type:  PERSON Name:  Python Core \n",
            "Type:  ORGANIZATION Name:  SoloLearn \n",
            "Type:  ORGANIZATION Name:  Mental Healthcare \n",
            "Type:  PERSON Name:  Speech Emotion \n",
            "Type:  PERSON Name:  Movie Recommender \n",
            "Type:  ORGANIZATION Name:  RAVDESS Dataset \n",
            "Type:  ORGANIZATION Name:  SER \n",
            "Type:  PERSON Name:  Web Scraping \n",
            "Type:  GPE Name:  Pytesseract \n",
            "Type:  ORGANIZATION Name:  NLP \n",
            "Type:  ORGANIZATION Name:  XG Boost \n",
            "Type:  PERSON Name:  Random Forest \n",
            "Type:  ORGANIZATION Name:  OCR \n",
            "Type:  GPE Name:  Handwritten \n",
            "Type:  ORGANIZATION Name:  CNN \n",
            "Type:  ORGANIZATION Name:  OpenCV \n",
            "Type:  ORGANIZATION Name:  TensorFlow \n",
            "Type:  PERSON Name:  Face Mask \n",
            "Type:  PERSON Name:  Haar Cascade Classifier \n",
            "Type:  ORGANIZATION Name:  OpenCV \n",
            "Type:  GPE Name:  Mask \n",
            "Type:  PERSON Name:  Mask \n",
            "Type:  PERSON Name:  Incorrect Mask \n",
            "Type:  ORGANIZATION Name:  LBPH Algorithm \n",
            "Type:  PERSON Name:  Heart Attack \n",
            "Type:  ORGANIZATION Name:  Insurance Company \n",
            "Type:  ORGANIZATION Name:  SKILLS \n",
            "Type:  PERSON Name:  Machine Learning \n",
            "Type:  ORGANIZATION Name:  TensorFlow \n",
            "Type:  ORGANIZATION Name:  OpenCV \n",
            "Type:  ORGANIZATION Name:  NumPy \n",
            "Type:  GPE Name:  Pytesseract \n",
            "Type:  PERSON Name:  Keras Experience \n",
            "Type:  PERSON Name:  Python \n",
            "Type:  ORGANIZATION Name:  JAVA \n",
            "Type:  ORGANIZATION Name:  HTML \n",
            "Type:  ORGANIZATION Name:  CSS \n",
            "Type:  ORGANIZATION Name:  JavaScript \n",
            "Type:  PERSON Name:  Data Structures \n",
            "Type:  ORGANIZATION Name:  SQL Software \n",
            "Type:  PERSON Name:  Jupyter Notebook \n",
            "Type:  PERSON Name:  Google Colab \n",
            "Type:  PERSON Name:  Code Blocks \n",
            "Type:  ORGANIZATION Name:  MATLAB \n",
            "Type:  PERSON Name:  Turbo C++ \n",
            "Type:  ORGANIZATION Name:  ECE Society \n",
            "Type:  ORGANIZATION Name:  BIT Mesra \n",
            "Type:  GPE Name:  Ranchi \n",
            "Type:  ORGANIZATION Name:  Participant \n",
            "Type:  PERSON Name:  Google Cloud \n",
            "Type:  GPE Name:  Brands \n",
            "Type:  ORGANIZATION Name:  SGGSCC \n",
            "Type:  ORGANIZATION Name:  University \n",
            "Type:  GPE Name:  Delhi \n",
            "Type:  GPE Name:  Street Play \n",
            "Type:  ORGANIZATION Name:  Intra Faculty Competition \n",
            "Type:  ORGANIZATION Name:  WORKSHOPS AND \n",
            "Type:  ORGANIZATION Name:  EVENTS \n",
            "Type:  GPE Name:  Rise \n",
            "Type:  GPE Name:  Crisis \n",
            "Type:  ORGANIZATION Name:  SRCC \n",
            "Type:  ORGANIZATION Name:  University \n",
            "Type:  GPE Name:  Delhi \n",
            "Type:  GPE Name:  Hero \n",
            "Type:  PERSON Name:  Campus Challenge S7 \n",
            "Type:  PERSON Name:  Hero \n",
            "Type:  ORGANIZATION Name:  MotoCorp Limited \n",
            "Type:  ORGANIZATION Name:  KLA Workshop \n",
            "Type:  ORGANIZATION Name:  HPC \n",
            "Type:  GPE Name:  Semiconductor \n",
            "Type:  ORGANIZATION Name:  IIT Madras \n",
            "Type:  ORGANIZATION Name:  NLP \n",
            "Type:  PERSON Name:  Disaster Tweets \n",
            "Type:  GPE Name:  Kaggle \n",
            "Type:  PERSON Name:  Qualified Vishleshan \n",
            "Type:  ORGANIZATION Name:  Analytics \n",
            "Type:  ORGANIZATION Name:  NIT Trichy \n",
            "Type:  GPE Name:  Prabandhan \n",
            "Type:  PERSON Name:  Annual Management Conclave \n",
            "Type:  ORGANIZATION Name:  IIT Kanpur \n",
            "Type:  ORGANIZATION Name:  Geospatial Inputs \n",
            "Type:  PERSON Name:  Master Plan \n",
            "Type:  GPE Name:  Indian \n",
            "Type:  ORGANIZATION Name:  Institute \n",
            "Type:  PERSON Name:  Remote Sensing \n",
            "Type:  PERSON Name:  Stage Management Team \n",
            "Type:  GPE Name:  Drama Fest \n",
            "Type:  ORGANIZATION Name:  District \n",
            "Type:  ORGANIZATION Name:  National Service Scheme \n",
            "Type:  ORGANIZATION Name:  NSS Camp \n",
            "Type:  ORGANIZATION Name:  INTERESTS \n",
            "Type:  PERSON Name:  Machine Learning \n",
            "Type:  PERSON Name:  Data Science \n",
            "Type:  PERSON Name:  Email \n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "from nltk import ne_chunk, pos_tag, word_tokenize\n",
        "from nltk.tree import Tree\n",
        "\n",
        "text = '''\n",
        "This is a sample text that contains the name Alex S William who is one of the developers of this project.\n",
        "You can also find the surname Jones here.\n",
        "'''\n",
        "\n",
        "nltk_results = ne_chunk(pos_tag(word_tokenize(txt)))\n",
        "for nltk_result in nltk_results:\n",
        "    if type(nltk_result) == Tree:\n",
        "        name = ''\n",
        "        for nltk_result_leaf in nltk_result.leaves():\n",
        "            name += nltk_result_leaf[0] + ' '\n",
        "        print ('Type: ', nltk_result.label(), 'Name: ', name)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 86.243766,
      "end_time": "2022-11-21T04:54:35.791851",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2022-11-21T04:53:09.548085",
      "version": "2.3.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}