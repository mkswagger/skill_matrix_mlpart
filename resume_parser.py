# -*- coding: utf-8 -*-
"""resume_parser.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LUxEaariQsI6Koe-M_rYCC1MqC2neMBu
"""



import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)



import os
for dirname, _, filenames in os.walk('/content/drive/MyDrive/resume_parser/pt2'):
    for filename in filenames:
        print(os.path.join(dirname, filename))



from google.colab import drive
drive.mount('/content/drive')

!pip install pdfminer-six

from pdfminer.pdfpage import PDFPage
from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter
from pdfminer.converter import TextConverter
from pdfminer.layout import LAParams
import io

"""**Parse PDF to TXT**"""

i_f = open('/content/drive/MyDrive/resume_parser/pt2/AnuvaGoyal_Latex.pdf','rb')
resMgr = PDFResourceManager()
retData = io.StringIO()
TxtConverter = TextConverter(resMgr,retData, laparams= LAParams())
interpreter = PDFPageInterpreter(resMgr,TxtConverter)
for page in PDFPage.get_pages(i_f):
    interpreter.process_page(page)
    txt = retData.getvalue()
    print(txt)

# extract email
extracted_text = {}
text = txt + "1234567891"
import re
def get_email_addresses(string):
    r = re.compile('[\w\.-]+\s*@\s*[\w\.-]+')
    return r.findall(string)

email = get_email_addresses(txt)
print(email)

# extract phone number
def get_phone_numbers(string):
    r = re.compile(r'(\d{3}[-\.\s]??\d{3}[-\.\s]??\d{4}|\(\d{3}\)\s*\d{3}[-\.\s]??\d{4}|\d{3}[-\.\s]??\d{4})')
    
    phone_numbers = r.findall(string)
    return [re.sub(r'\D', '', num) for num in phone_numbers]

phone_number= get_phone_numbers(txt)
print(phone_number)

!pip install nlp

# extract name

import spacy
from spacy.matcher import Matcher

# load pre-trained model
nlp = spacy.load('en_core_web_sm')

# initialize matcher with a vocab
matcher = Matcher(nlp.vocab)


def extract_name(resume_text):
    nlp_text = nlp(resume_text)
    
    # rule: name must be consisting of 2 proper noun (only first and last name)
    patterns = [{'POS': 'PROPN'}, {'POS': 'PROPN'}]
    
    matcher.add('NAME', [patterns], on_match = None)
    
    matches = matcher(nlp_text)
    
    for match_id, start, end in matches:
        span = nlp_text[start:end]
        return span.text

name = extract_name(txt)
print(name)
extracted_text["Name"] = name

# extract edu

import spacy
import pandas as pd

# load pre-trained model
nlp = spacy.load('en_core_web_sm')


def extract_edu(resume_text):
    nlp_text = nlp(resume_text)
    
    # word tokenization and remove stop words
    tokens = [token.text for token in nlp_text if not token.is_stop]
    
    # manually add skills database
    sets = ['b.e', 'b.tech', 'b.s', 'be', 
              'btech', 'bs', 'b.s.',
              'bachelor of science', 'bachelor of engineering',
              'bachelor of technology', 'bachelor of']
    
    extractedEdu = []
    
    # one-gram (example: B.E)
    for i in tokens:
        if i.lower() in sets: 
            extractedEdu.append(i)
    
    
    # bi-grams or tri-grams (example: bachelor of science)
    for i in nlp_text.noun_chunks:
        i = i.text.lower().strip()
        if i in sets:
            extractedEdu.append(i)

    
    # capitalize and remove duplicates
    return [word.capitalize() for word in set([word.lower() for word in extractedEdu])]
    
    

extract_edu(txt)

# load external database
result = []
with open('/content/drive/MyDrive/resume_parser/pt2/linkedin skill') as f:
    external_source = list(f)
    
for element in external_source:
    result.append(element.strip().lower())
    
#print(result)
#external_source = open("/kaggle/input/employment-skills/linkedin skill", "r")

#def Convert(string):
 #   li = list(string.split(" "))
  #  return li
#external_source = Convert(external_source)

## Extract Skill Ver 2.0 ##

import spacy
import pandas as pd

# load pre-trained model
nlp = spacy.load('en_core_web_sm')

result = []
with open('/content/drive/MyDrive/resume_parser/pt2/linkedin skill') as f:
    external_source = list(f)
    
for element in external_source:
    result.append(element.strip().lower())

import spacy

# load pre-trained model
nlp = spacy.load('en_core_web_sm')

def extract_skill_1(resume_text):
    nlp_text = nlp(resume_text)
    
    # word tokenization and remove stop words
    tokens = [token.text for token in nlp_text if not token.is_stop]
    
    # manually add skills database
    skills = result
    skillset = []
    
    # one-gram skill (example: python)
    for i in tokens:
        if i.lower() in skills: # make every skill lowercase to match with skills database we had
            skillset.append(i)
    
    
    # bi-grams or tri-grams skill (example: machine learning)
    for i in nlp_text.noun_chunks:
        i = i.text.lower().strip()
        if i in skills:
            skillset.append(i)

    
    # capitalize and remove duplicates
    return [word.capitalize() for word in set([word.lower() for word in skillset])]


extract_skill_1(txt)

#f = open("/kaggle/input/employment-skills/linkedin skill", "r")
#print(f.read())

import re
import spacy
from nltk.corpus import stopwords
import nltk
nltk.download('stopwords')

# load pre-trained model
nlp = spacy.load('en_core_web_sm')

# Grad all general stop words
STOPWORDS = set(stopwords.words('english'))

# Education Degrees
EDUCATION = [
            'BE','B.E.', 'B.E', 'BS', 'B.S', 
            'ME', 'M.E', 'M.E.', 'MS', 'M.S', 
            'BTECH', 'B.TECH', 'M.TECH', 'MTECH', 
            'SSC', 'HSC', 'CBSE', 'ICSE', 'BACHELOROFSCIENCE',
    'Bachelor of Science', 'BACHELOR OF SCIENCE'
        ]

def extract_education(resume_text):
    nlp_text = nlp(resume_text)

    # Sentence Tokenizer
    nlp_text = [sent.text.strip() for sent in nlp_text.sents]

    edu = {}
    # Extract education degree
    for index, text in enumerate(nlp_text):
        for tex in text.split():
            # Replace all special symbols
            tex = re.sub(r'[?|$|.|!|,]', r'', tex)
            if tex.upper() in EDUCATION and tex not in STOPWORDS:
                edu[tex] = text + nlp_text[index + 1]

    education = []
    for key in edu.keys():
        year = re.search(re.compile(r'(((20|19)(\d{2})))'), edu[key])
        if year:
            education.append((key, ''.join(year[0])))
        else:
            education.append(key)
    return education


education = extract_education(txt)
education

# extract skillset

import spacy

# load pre-trained model
nlp = spacy.load('en_core_web_sm')

def extract_skill(resume_text):
    nlp_text = nlp(resume_text)
    
    # word tokenization and remove stop words
    tokens = [token.text for token in nlp_text if not token.is_stop]
    
    # manually add skills database
    skills = ['python', 'machine learning', 
               'css', 'C++', 'data science',
               'PHP', 'mySQL', 'HTML', 'SQL',
             'tensorflow', 'deep learning',
             'pandas', 'opencv', 'typescript', 'c#',
              'data factory', 'ci/cd']
    
    skillset = []
    
    # one-gram skill (example: python)
    for i in tokens:
        if i.lower() in skills: # make every skill lowercase to match with skills database we had
            skillset.append(i)
    
    
    # bi-grams or tri-grams skill (example: machine learning)
    for i in nlp_text.noun_chunks:
        i = i.text.lower().strip()
        if i in skills:
            skillset.append(i)

    
    # capitalize and remove duplicates
    return [word.capitalize() for word in set([word.lower() for word in skillset])]
    
    

extract_skill(txt)

# extract degree

import re
import spacy
from nltk.corpus import stopwords

# load pre-trained model
nlp = spacy.load('en_core_web_sm')

# get all general stop words
# STOPWORDS = set(stopwords.words('english'))

DEGREES = ['b.e', 'b.tech', 'b.s', 'be', 
              'btech', 'bs', 'b.s.',
              'bachelor of science', 'bachelor of engineering',
              'bachelor of technology', 'bachelor of']


def extract_degree1(resume_text):
    nlp_text = nlp(resume_text)
    
    # word tokenization and remove stop words
    tokens = [token.text for token in nlp_text if not token.is_stop]
    
    
    
    degree = []
    
    for i in tokens:
        if i.lower() in DEGREES:
            degree.append(i)
    
    for i in nlp_text.noun_chunks:
        i = i.text.lower().strip()
        if i in DEGREES:
            degree.append(i)

            
    return [word.capitalize() for word in set([word.lower() for word in degree])]



extract_degree1(txt)

# extract school using regular expression (ver 1)

import re
def extract_school(resume_txt):
    sub_patterns = [
                    '[A-Za-zÀ-ȕ]* University', 
                    '[A-Za-zÀ-ȕ]* [A-Za-zÀ-ȕ]* University', 
                    '[A-Za-zÀ-ȕ]* [A-Za-zÀ-ȕ]* [A-Za-zÀ-ȕ]* University', 
                    #'[A-Za-zÀ-ȕ]* [A-Za-zÀ-ȕ]* [A-Za-zÀ-ȕ]* [A-Za-zÀ-ȕ]* University',
                    '[A-Za-zÀ-ȕ]* Institute [A-Za-zÀ-ȕ]* [A-Za-zÀ-ȕ]*',
                    '[A-Za-zÀ-ȕ]* [A-Za-zÀ-ȕ]* Institute *[A-Za-zÀ-ȕ] *[A-Za-zÀ-ȕ]',            
                    '[A-Za-zÀ-ȕ]* Institute',
                    '[A-Za-zÀ-ȕ]* [A-Za-zÀ-ȕ]* Institute',
                    '[A-Za-zÀ-ȕ]* [A-Za-zÀ-ȕ]* [A-Za-zÀ-ȕ]* Institute',
                    'Institute [A-Za-zÀ-ȕ]* [A-Za-zÀ-ȕ]*', 
                    'Institute [A-Za-zÀ-ȕ]* [A-Za-zÀ-ȕ]*',
                    'Institute [A-Za-zÀ-ȕ]*',    
                    'University *[A-Za-zÀ-ȕ] [A-Za-zÀ-ȕ]*',
                    'University [A-Za-zÀ-ȕ]* [A-Za-zÀ-ȕ]*',
                    'University [A-Za-zÀ-ȕ]*',
                    '[A-Za-zÀ-ȕ]* School', 
                    '[A-Za-zÀ-ȕ]* [A-Z][a-z]* School', 
                    '[A-Za-zÀ-ȕ]* [A-Za-zÀ-ȕ]* [A-Za-zÀ-ȕ]* School']
    pattern = '({})'.format('|'.join(sub_patterns))
    matches = re.findall(pattern, resume_txt)
    return(list(dict.fromkeys(matches)))

extract_school(txt)

# extract school name using external database (ver 2)

# import World University Ranking data
import pandas as pd
import re

df = pd.read_csv('/content/drive/MyDrive/resume_parser/pt2/WORLD UNIVERSITY RANKINGS.csv')

school_name = list(df['Institution'])

# txt = 'Harvard University ' + txt

# search and match school name with the database
def match_school(resume_text):
    school = []
    
    i = 0
    for i in range (len(school_name)):
        if school_name[i] in resume_text:
            school.append(school_name[i])
            i = i + 1
        else:
            i = i + 1
    
    return(school)

match_school(txt)

## sample 1

i_f = open('/content/drive/MyDrive/resume_parser/pt2/1901841_RESUME.pdf','rb')
resMgr = PDFResourceManager()
retData = io.StringIO()
TxtConverter = TextConverter(resMgr,retData, laparams= LAParams())
interpreter = PDFPageInterpreter(resMgr,TxtConverter)
for page in PDFPage.get_pages(i_f):
    interpreter.process_page(page)
    txt = retData.getvalue()
    
def extractResume(resume_text):
    
    email_address = get_email_addresses(resume_text)
    phone_number = get_phone_numbers(resume_text)
    name = extract_name(resume_text)
    skill = extract_skill(resume_text)
    skill_from_external = extract_skill_1(resume_text)
    degree = extract_education(resume_text)
    sch = extract_school(resume_text)

    getResult = {}
    getResult['email'] = email_address
    getResult['phone_number'] = phone_number
    getResult['name'] = name
    getResult['skill'] = skill
    getResult['skill_1'] = skill_from_external
    getResult['degree'] = degree
    getResult['sch'] = sch
    return(getResult)
# print(txt)

extractResume(txt)

## sample 2
# name prediction false

i_f_1 = open('/content/drive/MyDrive/resume_parser/inputs/Xinni_Chng.pdf','rb')
resMgr = PDFResourceManager()
retData = io.StringIO()
TxtConverter = TextConverter(resMgr,retData, laparams= LAParams())
interpreter = PDFPageInterpreter(resMgr,TxtConverter)
for page in PDFPage.get_pages(i_f_1):
    interpreter.process_page(page)
    txt_1 = retData.getvalue()
    
extractResume(txt_1)

## sample 3

i_f_2 = open("/content/drive/MyDrive/resume_parser/pt2/Arindam Lahiri Resume Aug '21.pdf",'rb')
resMgr = PDFResourceManager()
retData = io.StringIO()
TxtConverter = TextConverter(resMgr,retData, laparams= LAParams())
interpreter = PDFPageInterpreter(resMgr,TxtConverter)
for page in PDFPage.get_pages(i_f_2):
    interpreter.process_page(page)
    txt_2 = retData.getvalue()
    
extractResume(txt_2)

## sample 4

i_f_3 = open('/content/drive/MyDrive/resume_parser/pt2/Snehil_RA1911004010432.pdf','rb')
resMgr = PDFResourceManager()
retData = io.StringIO()
TxtConverter = TextConverter(resMgr,retData, laparams= LAParams())
interpreter = PDFPageInterpreter(resMgr,TxtConverter)
for page in PDFPage.get_pages(i_f_3):
    interpreter.process_page(page)
    txt_3 = retData.getvalue()
    
extractResume(txt_3)

!pip install nameparser

import nltk
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('maxent_ne_chunker')
nltk.download('words')

from nltk import ne_chunk, pos_tag, word_tokenize
from nltk.tree import Tree

text = '''
This is a sample text that contains the name Alex S William who is one of the developers of this project.
You can also find the surname Jones here.
'''

nltk_results = ne_chunk(pos_tag(word_tokenize(txt)))
for nltk_result in nltk_results:
    if type(nltk_result) == Tree:
        name = ''
        for nltk_result_leaf in nltk_result.leaves():
            name += nltk_result_leaf[0] + ' '
        print ('Type: ', nltk_result.label(), 'Name: ', name)
